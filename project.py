# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urShEdGM1N38dPOoInoskuSn9hBQvN-X

ENVIRONMENT SETUP
"""

# Install required libraries
# transformers: model loading & training
# datasets: dataset handling
# evaluate: evaluation metrics
# torch: deep learning backend
!pip install -q transformers datasets accelerate sentencepiece evaluate scikit-learn pandas numpy torch

# Import core libraries
import pandas as pd
import numpy as np
import torch

# Hugging Face utilities
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
    Trainer,
    TrainingArguments
)

"""LOAD AND PREPARE DATA"""

# Load the two CounselChat CSV datasets
df1 = pd.read_csv("/content/20220401_counsel_chat.csv")
df2 = pd.read_csv("/content/counselchat-data.csv")

# Combine both datasets row-wise
# This keeps all question–answer pairs together
df = pd.concat([df1, df2], ignore_index=True)

#display first few rows of joint data
df.head()

# Standardise column names so both datasets share the same schema
df = df.rename(columns={
    "questionText": "question",
    "answerText": "answer"
})

# Retain only the columns required for the project
# These directly support the problem → advice modelling task
# Remove rows with missing values
df = df[["question", "answer"]].dropna()

# Remove duplicate question–answer pairs
df = df.drop_duplicates()

"""LENGTH-BASED FILTERING"""

# Compute character length of questions and answers
# These helper columns are used only for filtering
df["question_len"] = df["question"].str.len()
df["answer_len"] = df["answer"].str.len()

# Remove outliers that are too long for transformer models
# This ensures stable training and concise advice generation
df = df[(df["question_len"] < 500) & (df["answer_len"] < 600)]

# Drop helper columns after filtering
df = df.reset_index(drop=True)

"""MENTAL HEALTH TEXT ANALYSIS MODEL

(BERT-based classifier)
"""

# We use a publicly available clinical BERT model
# This avoids gated access issues and is widely used in research
from transformers import AutoTokenizer, AutoModelForSequenceClassification
MODEL_NAME = "emilyalsentzer/Bio_ClinicalBERT"

# Load tokenizer for the mental-health analysis model
mental_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Load model for sequence classification
# We use 2 labels as a simplified setup (e.g., distress vs non-distress)
mental_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=2
)

"""Prepare Dataset for Mental Health Analysis"""

# Tokenisation function for BERT-style models
def mental_tokenize(batch):
    return mental_tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

"""PREPARE DATA FOR FLAN-T5

(Advice Generation)

"""

# Create a dataset for classification
# Since we do not have labelled distress data, labels are simulated
mental_df = pd.DataFrame({
    "text": df["question"],
    "label": np.random.randint(0, 2, size=len(df))  # proxy labels
})

# Convert to Hugging Face Dataset format
from datasets import Dataset
mental_dataset = Dataset.from_pandas(mental_df)

# Apply tokenisation
mental_dataset = mental_dataset.map(mental_tokenize, batched=True)

# Split into training and validation sets
mental_dataset = mental_dataset.train_test_split(test_size=0.2)

# Define training configuration
from transformers import TrainingArguments, Trainer

mental_args = TrainingArguments(
    output_dir="./mentalbert",
    eval_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    logging_steps=50,
    save_strategy="epoch",
    report_to="none"
)

# Create Trainer object
# NOTE: tokenizer is no longer passed directly to Trainer
mental_trainer = Trainer(
    model=mental_model,
    args=mental_args,
    train_dataset=mental_dataset["train"],
    eval_dataset=mental_dataset["test"]
)

# Train the model
mental_trainer.train()

# Convert question–answer pairs into instruction-based format
def format_flan(example):
    prompt = (
        "Provide empathetic, safe advice for the following concern:\n"
        f"{example['question']}"
    )
    return {
        "input_text": prompt,
        "target_text": example["answer"]
    }

# Apply formatting
flan_df = df.apply(format_flan, axis=1, result_type="expand")

# Convert to Hugging Face Dataset
flan_dataset = Dataset.from_pandas(flan_df)

# Split into training and validation sets
flan_dataset = flan_dataset.train_test_split(test_size=0.1)

"""LOAD FLAN-T5 MODEL"""

FLAN_MODEL = "google/flan-t5-small"

# Load tokenizer and model
flan_tokenizer = AutoTokenizer.from_pretrained(FLAN_MODEL)
flan_model = AutoModelForSeq2SeqLM.from_pretrained(FLAN_MODEL)

def flan_tokenize(batch):

  # Tokenise inputs
    model_inputs = flan_tokenizer(
        batch["input_text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

    # Tokenise targets (labels)
    labels = flan_tokenizer(
        batch["target_text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenisation
tokenized_flan = flan_dataset.map(flan_tokenize, batched=True, remove_columns=flan_dataset["train"].column_names)

"""TRAIN FLAN-T5"""

# Training configuration for Flan-T5
flan_args = TrainingArguments(
    output_dir="./flan_t5",
    eval_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    save_strategy="epoch",
    logging_steps=50,
    report_to="none",
    fp16=torch.cuda.is_available()
)

# Create Trainer for Flan-T5
flan_trainer = Trainer(
    model=flan_model,
    args=flan_args,
    train_dataset=tokenized_flan["train"],
    eval_dataset=tokenized_flan["test"],
    tokenizer=flan_tokenizer
)

# Train Flan-T5 model
flan_trainer.train()

# Save fine-tuned Flan-T5 model
flan_model.save_pretrained("./flan_t5")
flan_tokenizer.save_pretrained("./flan_t5")

"""INFERENCE PIPELINE"""

# Analyse emotional content using BERT-based model
def analyze_emotion(text):
    inputs = mental_tokenizer(text, return_tensors="pt", truncation=True)
    outputs = mental_model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    return probs.detach().numpy()

# Generate advice using Flan-T5
def generate_advice(user_input):
    emotion_score = analyze_emotion(user_input)

    instruction = (
        "Provide calm, supportive advice suitable for a young person.\n"
        f"User concern: {user_input}"
    )

    inputs = flan_tokenizer(instruction, return_tensors="pt")
    outputs = flan_model.generate(
        **inputs,
        max_length=120,
        temperature=0.7,
        do_sample=True
    )

    advice = flan_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return {
        "emotion_score": emotion_score,
        "ai_advice": advice
    }

# Test the full pipeline
test_input = "I feel overwhelmed by school and like I am not good enough."
result = generate_advice(test_input)

result

!pip install rouge_score

import evaluate

bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

preds = []
refs = []

for i in range(50):
    out = generate_advice(df["question"][i])
    preds.append(out["ai_advice"])
    refs.append([df["answer"][i]])

bleu.compute(predictions=preds, references=refs)

rouge.compute(predictions=preds, references=[r[0] for r in refs])